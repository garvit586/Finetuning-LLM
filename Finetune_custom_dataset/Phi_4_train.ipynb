{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e7ec9d-6f05-48d7-ac41-3c43fbf5c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import imagesize\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, RandomSampler, DataLoader, Sampler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import transformers\n",
    "# from transformers import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import bitsandbytes as bnb\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa76f6c-cc38-4bb9-83eb-4d0126eb1f8d",
   "metadata": {},
   "source": [
    "## Training using Plain instruction + example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbadaa8-4b30-41e0-a99b-bbfca03ea8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLoader(Dataset):\n",
    "    def __init__(self, sortocr=0, max_seq=1024, tokenizer=None, testmode=0):\n",
    "        self.dyn_max_seq_len = max_seq\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.testmode = testmode\n",
    "        self.sortocr = sortocr\n",
    "        self.data = self.load_cigroupdata()\n",
    "        print('self.data',len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        #return 1\n",
    "        \n",
    "    def load_cigroupdata(self):\n",
    "        samples = 0\n",
    "        dirp = '/mnt/efs/RaghavWork/CIGroups'\n",
    "        outsp = {}\n",
    "        for root, dir, files in os.walk(dirp):\n",
    "            for file in files:\n",
    "                if file.endswith('.json'):continue\n",
    "                key = os.path.basename(root)\n",
    "                fullpath = os.path.join(root, file)\n",
    "                fullpath_v2 = os.path.join(root, file+'_geo.json')\n",
    "                if all(map(lambda x:os.path.isfile(x), [fullpath, fullpath_v2])):\n",
    "                    if key not in outsp:outsp[key] = []\n",
    "                    #outsp[key].append((fullpath, fullpath_v2))\n",
    "                    outsp[key].append((fullpath, fullpath_v2))\n",
    "                    samples +=1\n",
    "                    \n",
    "        allkys = list(outsp.keys())\n",
    "        count = 0\n",
    "        datalist = []\n",
    "        for k in allkys:\n",
    "            if len(outsp[k])<2:\n",
    "                allkys.pop(k)\n",
    "            else:\n",
    "                count += len(outsp[k])\n",
    "                datalist.append(outsp.pop(k))\n",
    "                \n",
    "        print('Total Samples:',samples, ' | datalist',len(datalist))\n",
    "        if self.testmode:\n",
    "            return datalist[-20:]\n",
    "        else:\n",
    "            return datalist[:-20]\n",
    "\n",
    "    def sort_ocr_by_position(self, ocr_data):\n",
    "        \"\"\"\n",
    "        Sort OCR data by reading order (left to right, top to bottom).\n",
    "        \n",
    "        Args:\n",
    "        ocr_data (list): List of tuples in the format [((x1, y1, x3, y3), text)], where \n",
    "                         (x1, y1) is the top-left corner and (x3, y3) is the bottom-right corner of the word.\n",
    "        \n",
    "        Returns:\n",
    "        list: Sorted list of tuples by reading order.\n",
    "        \"\"\"\n",
    "        # Sort first by the y1 (top-to-bottom), then by x1 (left-to-right)\n",
    "        # Example OCR data\n",
    "        # ocr_data = [\n",
    "        #     ((100, 50, 150, 80), 'hello'),\n",
    "        #     ((200, 50, 250, 80), 'world'),\n",
    "        #     ((100, 100, 150, 130), 'foo'),\n",
    "        #     ((200, 100, 250, 130), 'bar')\n",
    "        # ]\n",
    "        \n",
    "        # # Sort OCR data\n",
    "        # sorted_data = sort_ocr_by_position(ocr_data)\n",
    "        \n",
    "        # Display the sorted OCR data\n",
    "        # for box, text in sorted_data:\n",
    "        #     print(f\"Text: {text}, Coordinates: {box}\")\n",
    "        \n",
    "        sorted_ocr = sorted(ocr_data, key=lambda item: (item[0][1], item[0][0]))\n",
    "    \n",
    "        combined_text = \" \".join([item[1] for item in sorted_ocr])\n",
    "        \n",
    "        return sorted_ocr, combined_text\n",
    "    \n",
    "    def get_image_and_jsongt(self, sample):\n",
    "        words = []\n",
    "        word_boxes = []\n",
    "        word_labels = []\n",
    "        row_labels = []\n",
    "        img = None\n",
    "        \n",
    "        imgp, jsonp =  sample\n",
    "        # if self.loadimage:\n",
    "        #     img = cv2.imread(imgp)\n",
    "        # else:img = None\n",
    "        img = None\n",
    "        if isinstance(jsonp, str) and os.path.isfile(jsonp):\n",
    "            with open(jsonp, encoding=\"utf8\") as f:\n",
    "                jsondata = json.load(f)\n",
    "        elif isinstance(jsonp, dict):jsondata = jsonp\n",
    "            \n",
    "    \n",
    "        for clas in jsondata['parse']['class']:\n",
    "            items = jsondata['parse']['class'][clas]\n",
    "            for item in items:\n",
    "                for wrd_id in item:\n",
    "                    word = jsondata['words'][wrd_id]['text']\n",
    "                    #print(f'Word: {word} --> {clas}')\n",
    "                    #words.append(word)\n",
    "                    #word_boxes.append(jsondata['words'][wrd_id]['boundingBox'][0] + jsondata['words'][wrd_id]['boundingBox'][2])\n",
    "                    #word_boxes.append(((jsondata['words'][wrd_id]['boundingBox'][0] + jsondata['words'][wrd_id]['boundingBox'][2]), word))\n",
    "                    #word_labels.append(clas)\n",
    "                    rlabel = jsondata['words'][wrd_id].get('row_label', [0])[0]\n",
    "                    #row_labels.append(min(49, rlabel))\n",
    "                    word_boxes.append(((jsondata['words'][wrd_id]['boundingBox'][0] + jsondata['words'][wrd_id]['boundingBox'][2]),\\\n",
    "                                       word, clas, rlabel))\n",
    "                    \n",
    "        #print('words', len(words),' | word_boxes:',len(word_boxes), ' | word_labels:',len(word_labels))\n",
    "    \n",
    "        return imgp, img ,words, word_boxes, word_labels, row_labels\n",
    "\n",
    "    def get_prompt(self, ocr_formatted_1, ocr_formatted_2, example_output):\n",
    "        # prompt = \"Extract the line-item json from the ocr information of the document.\"\n",
    "        prompt = 'Your task is to extract line-items information from the attached invoice. Refer to the example \\\n",
    "        input and output and then return the output for the new input.'\n",
    "        \n",
    "        one_shot_prompt = f\"\"\"\n",
    "            Task: {prompt}\n",
    "            \n",
    "            Example:\n",
    "            Input: {ocr_formatted_1}\n",
    "            Output: {example_output}\n",
    "            \n",
    "            New Input: {ocr_formatted_2}\n",
    "            Output:\n",
    "            \"\"\"\n",
    "        return one_shot_prompt\n",
    "    \n",
    "    def get_final_json(self, sample):\n",
    "        row_wise_data = {'keyvalues':{}, 'lineitems':{}}\n",
    "        kvf = 1\n",
    "        for item in sample:\n",
    "            bx, word, cls, row = item\n",
    "            if row == 0:\n",
    "                if cls in ('PO_NUMBER_VALUE', 'HTS_NUMBER_VALUE', 'INVOICE_NUMBER_VALUE'):\n",
    "                    #if cls not in row_wise_data['keyvalues']:row_wise_data['keyvalues'][cls] = []\n",
    "                    #row_wise_data['keyvalues'][cls].append(word)\n",
    "                    if cls not in row_wise_data['keyvalues']:row_wise_data['keyvalues'][cls] = ''\n",
    "                    row_wise_data['keyvalues'][cls] += word + ' '\n",
    "                    kvf = 0\n",
    "                continue\n",
    "            if not cls.endswith('_VALUE') or 'MISC' in cls:continue\n",
    "            if row not in row_wise_data['lineitems']:row_wise_data['lineitems'][row] = {}\n",
    "            # if cls not in row_wise_data['lineitems'][row]:row_wise_data['lineitems'][row][cls] = []\n",
    "            # row_wise_data['lineitems'][row][cls].append(word)\n",
    "            if cls not in row_wise_data['lineitems'][row]:row_wise_data['lineitems'][row][cls] = ''\n",
    "            row_wise_data['lineitems'][row][cls] += word + ' '\n",
    "        \n",
    "        if kvf:row_wise_data.pop('keyvalues')\n",
    "        for row in row_wise_data['lineitems']:\n",
    "            for cls in row_wise_data['lineitems'][row]:\n",
    "                row_wise_data['lineitems'][row][cls] = row_wise_data['lineitems'][row][cls].strip()\n",
    "                \n",
    "        return row_wise_data\n",
    "    \n",
    "    def __getitem__(self, idx, testing_sample=0):\n",
    "        #idx = np.random.randint(0,len(self.data)))\n",
    "        sample1, sample2 = random.choices(self.data[idx], k = 2)\n",
    "        #sample1, sample2 = self.data[idx][0], self.data[idx][1]\n",
    "        #print('sample1, sample2',sample1, sample2)\n",
    "        imgp1, img1 ,words1, word_boxes1, word_labels1, row_labels1 = self.get_image_and_jsongt(sample1)\n",
    "        imgp2, img2 ,words2, word_boxes2, word_labels2, row_labels2 = self.get_image_and_jsongt(sample2)\n",
    "        \n",
    "        if self.sortocr:\n",
    "            word_boxes1, ocrtxt1 = self.sort_ocr_by_position(word_boxes1)\n",
    "            word_boxes2, ocrtxt2 = self.sort_ocr_by_position(word_boxes2)\n",
    "        else:\n",
    "            ocrtxt1 = ' '.join([item[1] for item in word_boxes1])\n",
    "            ocrtxt2 = ' '.join([item[1] for item in word_boxes2])\n",
    "\n",
    "        outjson1 = self.get_final_json(word_boxes1)\n",
    "        outjson2 = self.get_final_json(word_boxes2)\n",
    "        input_prompt = self.get_prompt(ocrtxt1, ocrtxt2, outjson1)\n",
    "        output = f\"{outjson2}\"\n",
    "        if not testing_sample:\n",
    "            #print('input_prompt--->',input_prompt)\n",
    "            #print('output--->',output)\n",
    "            combined_text = input_prompt + output\n",
    "            tokens = self.tokenizer(\n",
    "                combined_text, \n",
    "                max_length=self.dyn_max_seq_len, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=\"max_length\",\n",
    "                padding_side='right'\n",
    "            )\n",
    "            input_length = len(self.tokenizer(input_prompt)[\"input_ids\"])\n",
    "            labels = tokens[\"input_ids\"].clone()\n",
    "            labels[:, :input_length] = -100  # Ignore input tokens in the loss\n",
    "            labels[tokens[\"attention_mask\"]==0] = -100 # Ignore pad tokens in the loss\n",
    "            \n",
    "            #return {'input_prompt':input_prompt, 'output':output}\n",
    "            return {\n",
    "                \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "                \"labels\": labels.squeeze(0)\n",
    "            }\n",
    "        else:\n",
    "            return {'input_prompt':input_prompt, 'output':output, 'imgp1':imgp1, 'imgp2':imgp2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4340dce-0e2b-4c53-a03e-4c3dcef20075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer\n",
    "\n",
    "local_cache = './phi_4_model/'\n",
    "name = \"microsoft/Phi-4-mini-instruct\"\n",
    "# local_cache = './mistral_models'\n",
    "# name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "#http://localhost:8889/edit/mnt/efs/RaghavWork/VirtualENV/1shot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, cache_dir=local_cache)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # For causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca852816-9c14-4057-bdb1-e5188d42bdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bae0c1-75a0-4339-99da-ed3e0f96374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    cache_dir=local_cache,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
